{
 "metadata": {
  "name": "",
  "signature": "sha256:7e562b05f3396023ba63a47aa5d22b9dde4be411d2787f5acfe0cc91267772a6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# coding=utf-8\n",
      "from twitter import *\n",
      "\"\"\"\n",
      "there would be code here to set up a dictionary including everything required to load the authentication for twitter.\n",
      "What are the advantages of setting up a dictionary here instead of just defining it below?\n",
      "\"\"\"\n",
      "t = Twitter(\n",
      "    auth=OAuth(\n",
      "        token=config['TOKEN'],\n",
      "        token_secret=config['TOKEN_SECRET'],\n",
      "        consumer_key=config['CONSUMER_KEY'],\n",
      "        consumer_secret=config['CONSUMER_SECRET'])\n",
      "    )\n",
      "\n",
      "hashtags=[\n",
      "    u'hongkong',\n",
      "    u'occupycentral',\n",
      "    u'umbrellarevolution',\n",
      "    u'china',\n",
      "    u'hk',\n",
      "    u'admiralty',\n",
      "    u'occupyhk',\n",
      "]\n",
      "\n",
      "all_results = []\n",
      "for hashtag in hashtags:\n",
      "    results = t.search.tweets(q='#'+hashtag, count=100, result_type='mixed')\n",
      "    for r in results['statuses']:\n",
      "        try:\n",
      "            clean_tweet = unicode(r['text']).encode('utf-8').replace('\\n', ' ').replace('\\r', ' ').replace('\"', \"'\")\n",
      "            print u','.join([unicode(r['id']),'\"'+clean_tweet+'\"', '\"'+hashtag+'\"'])\n",
      "        except UnicodeDecodeError:\n",
      "            pass\n",
      "        \n",
      "        \n",
      "        \n",
      "#script using stream api  to db to gen csv\n",
      "def tweet_uniqueness(tweets):\n",
      "    # Code for finding the number of unique tweets in a column over the number of tweets.\n",
      "    # should return a number between 0 and 1\n",
      "    return float(len(set(tweets))) / float(len(tweets))\n",
      "\n",
      "# shows that with the code above, we didn't get completely unique tweets.\n",
      "print tweet_uniqueness(tweets.id) \n",
      "\n",
      "# code to drop duplicates based on the id column alone.\n",
      "unique_tweets = tweets.drop_duplicates(cols=['id'])\n",
      "\n",
      "print len(unique_tweets)\n",
      "print tweet_uniqueness(unique_tweets.id)\n",
      "print tweet_uniqueness(unique_tweets.tweet)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "import nltk\n",
      "\n",
      "def tokenize_tweet(t, remove_stop=True, remove_hashtag=False):\n",
      "    import string\n",
      "    import re\n",
      "    tweet = t\n",
      "    tweet = tweet.lower()\n",
      "    tweet = re.sub('@\\w+', 'TWITTER_HANDLE', tweet)\n",
      "    tweet = re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?', 'URL', tweet)\n",
      "    tweet = tweet.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
      "    words = nltk.tokenize.wordpunct_tokenize(tweet)\n",
      "    if remove_stop:\n",
      "        # How do we filter for words in the stopwords corpus?\n",
      "        stopwords_filter = set(nltk.corpus.stopwords.words('english'))\n",
      "        #list comprehension\n",
      "        words = [w for w in words if w not in stopwords_filter]\n",
      "\n",
      "    if remove_hashtag:\n",
      "        # How do we filter out the actual hashtag in the tweet itself?\n",
      "        words = [w for w in words if re.match('#\\w', w)]\n",
      "\n",
      "    return words\n",
      "\n",
      "\n",
      "unique_tweets['tokens'] = unique_tweets.tweet.apply(tokenize_tweet, remove_stop=True)\n",
      "unique_tweets['tokens_w_stopwords'] = unique_tweets.tweet.apply(tokenize_tweet, remove_stop=False)\n",
      "\n",
      "print unique_tweets['tokens_w_stopwords']\n",
      "\n",
      "#sentiment\n",
      "#iterate for adjective\n",
      "for w in words:\n",
      "    if w in adjective_list.keys():\n",
      "        sentiment_list.append(adjective_list)\n",
      "#if find, add sentiment to list\n",
      "\n",
      "#"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}