{
 "metadata": {
  "name": "",
  "signature": "sha256:31e6bf9eb69c68e08a908d965d4dc3d70a92c36c2d33ad16d3ab0942b6d69a66"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Naive Bayes Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Topics in this Notebook:\n",
      "* Naive Bayes Classifiers\n",
      "* Train Test Split\n",
      "* Vectorizer (fit and transform)\n",
      "* Accuracy of training and test data\n",
      "* Sparse array vs normal array\n",
      "* Swapping classifiers in sklearn\n",
      "\n",
      "### Overview\n",
      "\n",
      "* Look at Naive Bayes theory\n",
      "* Look at some example code\n",
      "* Try our own\n",
      "\n",
      "\n",
      "**Resources include:**\n",
      "* [SKL Naive Bayes Documentation](http://scikit-learn.org/stable/modules/naive_bayes.html)\n",
      "* [Stanford Naive Bayes Math](http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "## A Classic Naive Bayes Example (80% of Doctors get this wrong):\n",
      "1% of women at age forty who participate in routine screening have breast cancer.  80% of women with breast cancer will get positive mammographies.  9.6% of women without breast cancer will also get positive mammographies.  A woman in this age group had a positive mammography in a routine screening.  \n",
      "\n",
      "What is the probability that she actually has breast cancer?\n",
      "\n",
      "#### Take a moment to write out the answer\n",
      "\n",
      "<!--\n",
      "* Prior: 1% of women at age forty have breast cancer.\n",
      "* Posterior: Probability woman has breast cancer\n",
      "-->"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###What's the importance of the Naive Bayes Equation -- what do we need to know?\n",
      "   * **Why Naive? It assumes all inputs (categories, predictors) are independent**: In the case of text: its blind to word context and meaning: \n",
      "   * **Naivete helps because negligible information is required** -- simply word counting and that's it -- faster testing and very simple construction on ANY text.  Naivete doesn't hurt because correctness is based on classification, not prediction.\n",
      "   * **Good Predictor, (possibly) Bad Estimator:** NB chooses among possible categories to find the one that is the greatest.  The associated probability assigned is not necessarily accurate.  For example -- it might predict an email is Spam at 80%, but the true value might be 95% -- this doesn't matter, since its just choosing the category, anything over 50% will give it a perfect score. So while good estimation means good prediction,  good prediction doesn't necessarily mean good estimation.\n",
      "   * **Bayes Theorem is Easily Derivable**: The best method I've found is Venn diagrams via [Oscar Bonilla](http://oscarbonilla.com/2009/05/visualizing-bayes-theorem/) ![](http://note.io/1nf77lD).  These two steps, clear from the diagram above, can do it:\n",
      "      \n",
      "* P(AB) = |AB| / |U| \n",
      "* P(A|B) = |AB| / |B|\n",
      "\n",
      "       \n",
      "We then do some substitution...  \n",
      "\n",
      "* P(B|A) = P(AB)P(B) / P(A)\n",
      "* P(B|A) = P(A|B)P(B) / P(B)\n",
      "         \n",
      "         \n",
      "  * **Prior Knowledge is a Shortcut to Prediction** This is the philosophical element of Bayes that is important -- it uses prior knowledge.  That's the whole point of the \"A given B\" element of it.\n",
      "  \n",
      "  * Posterior =  likelihood function * prior probability / normalization constant\n",
      "    \n",
      "    \n",
      "    \n",
      "###What's the difference between Multinomial and Bernoulli Naive Bayes?\n",
      "   * Try Bernoulli when your dictionary of words and document length is small.\n",
      "   * With Bernoulli, you may add in additional non-word features easily\n",
      "   * Multinomial usually performs better, takes into account frequency vs. presence\n",
      "   * Consider the probability for the prior of the term \"the\" -- Bernoulli: 100%; Multinomial 5%\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### A great explanation of Bayes Classification\n",
      "*from [Mathias Brandewinder](http://clear-lines.com/blog/post/Naive-Bayes-Classification.aspx)*\n",
      "\n",
      "The canonical application of Bayes na\u00efve classification is in text classification, where the goal is to identify to which pre-determined category a piece of text belongs to  \u2013 for instance, is this email I just received spam, or ham (\u201cvaluable\u201d email)?\n",
      "\n",
      "The underlying idea is to use individual words present in the text as indications for what category it is most likely to belong to, using Bayes Theorem, named after the cheerful-looking Reverend Bayes.\n",
      "\n",
      "Imagine that you received an email containing the words \u201cNigeria\u201d, \u201cPrince\u201d, \u201cDiamonds\u201d and \u201cMoney\u201d. It is very likely that if you look into your spam folder, you\u2019ll find quite a few emails containing these words, whereas, unless you are in the business of importing diamonds from Nigeria and have some aristocratic family, your \u201cnormal\u201d emails would rarely contain these words. They have a much higher frequency within the category \u201cSpam\u201d than within the Ham, which makes them a potential flag for undesired business ventures.\n",
      "\n",
      "On the other hand, let\u2019s assume that you are a lucky person, and that typically, what you receive is Ham, with the occasional Spam bit. If you took a random email in your inbox, it is then much more likely that it belongs to the Ham category.\n",
      "\n",
      "Bayes\u2019 Theorem combines these two pieces of information together, to determine the probability that a particular email belongs to the \u201cSpam\u201d category, if it contains the word \u201cNigeria\u201d:\n",
      "\n",
      "P(is \u201cSpam\u201d|contains \u201dNigeria\u201d) = P(contains \u201cNigeria|is \u201dSpam\u201d) x P(is \u201cSpam\u201d) / P(contains \u201cNigeria\u201d)\n",
      "\n",
      "In other words, 2 factors should be taken into account when deciding whether an email containing \u201cNigeria\u201d is spam: how over-represented is that word in Spam, and how likely is it that any email is spammy in the first place?\n",
      "\n",
      "The algorithm is named \u201cNa\u00efve\u201d, because it makes a simplifying assumption about the text, which turns out to be very convenient for computations purposes, namely that each word appears with a frequency which doesn\u2019t depend on other words. This is an unlikely assumption (the word \u201cDiamond\u201d is much more likely to be present in an email containing \u201cNigeria\u201d than in your typical family-members discussion email).\n",
      "\n",
      "We\u2019ll leave it at that on the concepts \u2013  I\u2019ll refer the reader who want to dig deeper to the book, or to this explanation of text classification with Na\u00efve Bayes.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Supervised Classification Framework\n",
      "![](http://note.io/1tFLyia)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Read and Explore the Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import json\n",
      "\n",
      "import requests\n",
      "import sklearn\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 30)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that Kaggle is running a [competition](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data) on accurately rating movies using this dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "critics = pd.read_csv('rt_critics.csv')\n",
      "critics.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>critic</th>\n",
        "      <th>fresh</th>\n",
        "      <th>imdb</th>\n",
        "      <th>publication</th>\n",
        "      <th>quote</th>\n",
        "      <th>review_date</th>\n",
        "      <th>rtid</th>\n",
        "      <th>title</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>        Derek Adams</td>\n",
        "      <td> fresh</td>\n",
        "      <td> 114709</td>\n",
        "      <td>       Time Out</td>\n",
        "      <td> So ingenious in concept, design and execution ...</td>\n",
        "      <td> 2009-10-04</td>\n",
        "      <td> 9559</td>\n",
        "      <td> Toy story</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>    Richard Corliss</td>\n",
        "      <td> fresh</td>\n",
        "      <td> 114709</td>\n",
        "      <td>  TIME Magazine</td>\n",
        "      <td>                 The year's most inventive comedy.</td>\n",
        "      <td> 2008-08-31</td>\n",
        "      <td> 9559</td>\n",
        "      <td> Toy story</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>        David Ansen</td>\n",
        "      <td> fresh</td>\n",
        "      <td> 114709</td>\n",
        "      <td>       Newsweek</td>\n",
        "      <td> A winning animated feature that has something ...</td>\n",
        "      <td> 2008-08-18</td>\n",
        "      <td> 9559</td>\n",
        "      <td> Toy story</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>      Leonard Klady</td>\n",
        "      <td> fresh</td>\n",
        "      <td> 114709</td>\n",
        "      <td>        Variety</td>\n",
        "      <td> The film sports a provocative and appealing st...</td>\n",
        "      <td> 2008-06-09</td>\n",
        "      <td> 9559</td>\n",
        "      <td> Toy story</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> Jonathan Rosenbaum</td>\n",
        "      <td> fresh</td>\n",
        "      <td> 114709</td>\n",
        "      <td> Chicago Reader</td>\n",
        "      <td> An entertaining computer-generated, hyperreali...</td>\n",
        "      <td> 2008-03-10</td>\n",
        "      <td> 9559</td>\n",
        "      <td> Toy story</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "               critic  fresh    imdb     publication                                              quote review_date  rtid      title\n",
        "0         Derek Adams  fresh  114709        Time Out  So ingenious in concept, design and execution ...  2009-10-04  9559  Toy story\n",
        "1     Richard Corliss  fresh  114709   TIME Magazine                  The year's most inventive comedy.  2008-08-31  9559  Toy story\n",
        "2         David Ansen  fresh  114709        Newsweek  A winning animated feature that has something ...  2008-08-18  9559  Toy story\n",
        "3       Leonard Klady  fresh  114709         Variety  The film sports a provocative and appealing st...  2008-06-09  9559  Toy story\n",
        "4  Jonathan Rosenbaum  fresh  114709  Chicago Reader  An entertaining computer-generated, hyperreali...  2008-03-10  9559  Toy story"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Break - Take some time to explore the data.  What did you find out?  Have a graph to share?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Naive Bayes Classification"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.naive_bayes import MultinomialNB\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# \"Beijing and Taipei join the WTO\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Multinomial vs. Bernoulli:\n",
      "![](http://note.io/1ftU7X6)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Multinomial Model actually counts occurences out of all possible occurences for probability - better for greater features\n",
      "# Bernoulli model counts only all documents with presence of the word - better for fewer features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Vectorizing with SK-Learn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "### How the Count Vectorizer Works\n",
      "#\n",
      "\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "text = ['Math is great', 'Math is really great', 'Exciting exciting Math']\n",
      "print \"Original text is\\n\", '\\n'.join(text)\n",
      "\n",
      "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
      "\n",
      "# call `fit` to build the vocabulary\n",
      "vectorizer.fit(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Original text is\n",
        "Math is great\n",
        "Math is really great\n",
        "Exciting exciting Math\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 116,
       "text": [
        "CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
        "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "        tokenizer=None, vocabulary=None)"
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer.get_feature_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 117,
       "text": [
        "[u'exciting',\n",
        " u'exciting exciting',\n",
        " u'exciting math',\n",
        " u'great',\n",
        " u'is',\n",
        " u'is great',\n",
        " u'is really',\n",
        " u'math',\n",
        " u'math is',\n",
        " u'really',\n",
        " u'really great']"
       ]
      }
     ],
     "prompt_number": 117
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# call `transform` to convert text to a bag of words\n",
      "x = vectorizer.transform(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(x) # A compressed version"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 3)\t1\n",
        "  (0, 4)\t1\n",
        "  (0, 5)\t1\n",
        "  (0, 7)\t1\n",
        "  (0, 8)\t1\n",
        "  (1, 3)\t1\n",
        "  (1, 4)\t1\n",
        "  (1, 6)\t1\n",
        "  (1, 7)\t1\n",
        "  (1, 8)\t1\n",
        "  (1, 9)\t1\n",
        "  (1, 10)\t1\n",
        "  (2, 0)\t2\n",
        "  (2, 1)\t1\n",
        "  (2, 2)\t1\n",
        "  (2, 7)\t1\n"
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
      "# convert back to a \"normal\" numpy array\n",
      "x_back = x.toarray()\n",
      "x_back"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 107,
       "text": [
        "array([[0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0],\n",
        "       [0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1],\n",
        "       [2, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0]])"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print\n",
      "print \"Transformed text vector is \\n\", x\n",
      "\n",
      "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
      "print\n",
      "print \"Words for each feature:\"\n",
      "print vectorizer.get_feature_names()\n",
      "\n",
      "# Notice that the bag of words treatment doesn't preserve information about the *order* of words, \n",
      "# just their frequency"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Transformed text vector is \n",
        "  (0, 3)\t1\n",
        "  (0, 4)\t1\n",
        "  (0, 5)\t1\n",
        "  (0, 7)\t1\n",
        "  (0, 8)\t1\n",
        "  (1, 3)\t1\n",
        "  (1, 4)\t1\n",
        "  (1, 6)\t1\n",
        "  (1, 7)\t1\n",
        "  (1, 8)\t1\n",
        "  (1, 9)\t1\n",
        "  (1, 10)\t1\n",
        "  (2, 0)\t2\n",
        "  (2, 1)\t1\n",
        "  (2, 2)\t1\n",
        "  (2, 7)\t1\n",
        "\n",
        "Words for each feature:\n",
        "[u'exciting', u'exciting exciting', u'exciting math', u'great', u'is', u'is great', u'is really', u'math', u'math is', u'really', u'really great']\n"
       ]
      }
     ],
     "prompt_number": 118
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Preparing our Features (X) and Target (Y) for Training"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "    \n",
      " * X is a `(nreview, nwords)` array. Each row corresponds to a bag-of-words representation for a single review. This will be the *input* to the model.\n",
      " * Y is a `nreview`-element 1/0 array, encoding whether a review is Fresh (1) or Rotten (0). This is the desired *output* \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "critics.quote[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "'A winning animated feature that has something for everyone on the age spectrum.'"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a vector where each row is bag-of-words for a single quote\n",
      "X = vectorizer.fit_transform(critics.quote) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 249
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#temp\n",
      "rotten_vectorizer = vectorizer.fit(critics.quote)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 162
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create an array where each element encodes whether the array is Fresh or Rotten\n",
      "Y = (critics.fresh == 'fresh').values.astype(np.int)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 120
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "critics.quote[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 121,
       "text": [
        "\"The year's most inventive comedy.\""
       ]
      }
     ],
     "prompt_number": 121
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The Train/Test Split"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_test_split?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 122
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use SKLearn's train_test_split \n",
      "xtrain, xtest, ytrain, ytest = train_test_split(X, Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 124
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Creating the Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Not much code here\n",
      "\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "print \"MultinomialNB:\"\n",
      "clf = MultinomialNB().fit(xtrain, ytrain)\n",
      "accuracy_report(clf)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Basic Accuracy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def accuracy_report(_clf):\n",
      "    print \"Accuracy: %0.2f%%\" % (100 * _clf.score(xtest, ytest))\n",
      "\n",
      "    #Print the accuracy on the test and training dataset\n",
      "    training_accuracy = _clf.score(xtrain, ytrain)\n",
      "    test_accuracy = _clf.score(xtest, ytest)\n",
      "\n",
      "    print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
      "    print \"Accuracy on test data:     %0.2f\" % (test_accuracy)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 128
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How much [overfitting](http://en.wikipedia.org/wiki/Overfitting) is too much?  How do [we stop it](http://scikit-learn.org/stable/modules/cross_validation.html)?  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Swapping out cartridges for our black box"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import BernoulliNB\n",
      "print \"BernoulliNB:\"\n",
      "clf = BernoulliNB().fit(xtrain, ytrain)\n",
      "accuracy_report(clf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "BernoulliNB:\n",
        "Accuracy: 67.71%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Accuracy on training data: 0.87"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Accuracy on test data:     0.68\n"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "print \"Logistic Regression:\"\n",
      "clf = LogisticRegression().fit(xtrain, ytrain)\n",
      "accuracy_report(clf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Logistic Regression:\n",
        "Accuracy: 77.37%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Accuracy on training data: 1.00\n",
        "Accuracy on test data:     0.77\n"
       ]
      }
     ],
     "prompt_number": 135
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### A function to run some tests"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def AnalyzeReview(testquote):\n",
      "    print \"\\\"\"  + testquote + \"\\\" is judged by clasifier to be...\"\n",
      "    testquote = rotten_vectorizer.transform([testquote])\n",
      "\n",
      "    if (clf.predict(testquote)[0] == 1):\n",
      "        print \"... a fresh review.\"\n",
      "    else:\n",
      "        print \"... a rotten review.\"\n",
      "    return(clf.predict(testquote)[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 266
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "AnalyzeReview(\"This movie was awesome\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\"This movie was awesome\" is judged by clasifier to be...\n",
        "... a fresh review.\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 269,
       "text": [
        "1"
       ]
      }
     ],
     "prompt_number": 269
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Where does the classifier go wrong?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Save prediction and probability\n",
      "\n",
      "# Outputs of X (just first column)\n",
      "prob = clf.predict_proba(X)[:, 0]\n",
      "\n",
      "predict = clf.predict(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Y==0 #(provides a mask where the actual review is bad)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 241,
       "text": [
        "array([False, False, False, ..., False, False, False], dtype=bool)"
       ]
      }
     ],
     "prompt_number": 241
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# argsort returns the positions of the top n sorted values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.argsort((prob[Y==0]))[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 244,
       "text": [
        "array([2014,  756, 1647,  941, 4174])"
       ]
      }
     ],
     "prompt_number": 244
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Review errors\n",
      "bad_rotten = np.argsort(prob[Y == 0])[:5]\n",
      "bad_fresh = np.argsort(prob[Y == 1])[-5:]\n",
      "\n",
      "print \"Mis-predicted Rotten quotes\"\n",
      "print '---------------------------'\n",
      "for row in bad_rotten:\n",
      "    print critics[Y == 0].quote.irow(row)\n",
      "    print\n",
      "\n",
      "print \"Mis-predicted Fresh quotes\"\n",
      "print '--------------------------'\n",
      "for row in bad_fresh:\n",
      "    print critics[Y == 1].quote.irow(row)\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mis-predicted Rotten quotes\n",
        "---------------------------\n",
        "Has no one else found it highly peculiar that damn near everybody's choice for the best movie of (let's say) the decade should be dedicated, inferentially but absolutely, to the proposition that Courage is Madness and Cowardice is Best?"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "The performances are precise, the language is alive and well spoken and the setting is striking, but Vanya on 42nd Street still suffers rather heavily from the limitations of filmed theater.\n",
        "\n",
        "Lacking the surface glitz, attention-grabbing plot and star power of John Grisham's previous adaptations, Folley's film will also suffer due to comparisons with the similarly-themed and better Tim Robbins' Dead Man Walking, released last year.\n",
        "\n",
        "The fact that so much money was lavished on such a thick-headed project represents the height of fiscal ineptitude, but this is the kind of roller-coaster ride for which eager audiences will gladly check their brains at the turnstiles, so go figure.\n",
        "\n",
        "This is a film about deeply confused people that seems likely to put viewers in a state of deep confusion for most of its running time.\n",
        "\n",
        "Mis-predicted Fresh quotes\n",
        "--------------------------\n",
        "A good half-hour's worth of nonsense in the middle keeps Bad Boys from being little better than a break- even proposition.\n",
        "\n",
        "Though it's a good half hour too long, this overblown 1993 spin-off of the 60s TV show otherwise adds up to a pretty good suspense thriller.\n",
        "\n",
        "'Once Upon a Time...' now looks like an over-cooked mess of style, metaphor and reference.\n",
        "\n",
        "The movie's own payoff is compelling enough, but the project has a weightless feel that limits involvement. Better you give it an hour-and-a-half on video someday, surrounded by wine and snacks.\n",
        "\n",
        "There's too much talent and too strong a story to mess it up. There was potential for more here, but this incarnation is nothing to be ashamed of, and some of the actors answer the bell.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 214
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "critics[critics['quote'].str.contains(\"audacious in its\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>critic</th>\n",
        "      <th>fresh</th>\n",
        "      <th>imdb</th>\n",
        "      <th>publication</th>\n",
        "      <th>quote</th>\n",
        "      <th>review_date</th>\n",
        "      <th>rtid</th>\n",
        "      <th>title</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>7658</th>\n",
        "      <td> Mick LaSalle</td>\n",
        "      <td> rotten</td>\n",
        "      <td> 386117</td>\n",
        "      <td> San Francisco Chronicle</td>\n",
        "      <td> Where the Wild Things Are is audacious in its ...</td>\n",
        "      <td> 2009-10-16</td>\n",
        "      <td> 770671948</td>\n",
        "      <td> Where the Wild Things Are</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 131,
       "text": [
        "            critic   fresh    imdb              publication                                              quote review_date       rtid                      title\n",
        "7658  Mick LaSalle  rotten  386117  San Francisco Chronicle  Where the Wild Things Are is audacious in its ...  2009-10-16  770671948  Where the Wild Things Are"
       ]
      }
     ],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "critics[critics['quote'].str.contains(\"own payoff is\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>critic</th>\n",
        "      <th>fresh</th>\n",
        "      <th>imdb</th>\n",
        "      <th>publication</th>\n",
        "      <th>quote</th>\n",
        "      <th>review_date</th>\n",
        "      <th>rtid</th>\n",
        "      <th>title</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>5965</th>\n",
        "      <td> Mike Clark</td>\n",
        "      <td> fresh</td>\n",
        "      <td> 115710</td>\n",
        "      <td> USA Today</td>\n",
        "      <td> The movie's own payoff is compelling enough, b...</td>\n",
        "      <td> 2000-01-01</td>\n",
        "      <td> 364525542</td>\n",
        "      <td> Blood and Wine</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 132,
       "text": [
        "          critic  fresh    imdb publication                                              quote review_date       rtid           title\n",
        "5965  Mike Clark  fresh  115710   USA Today  The movie's own payoff is compelling enough, b...  2000-01-01  364525542  Blood and Wine"
       ]
      }
     ],
     "prompt_number": 132
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Your Turn\n",
      "\n",
      "Create a twitter classifier with naive bayes and check the sentiment for a keyword of your choice.\n",
      "\n",
      "You'll need your own twitter api key.  \n",
      "\n",
      "For the training set, you can use the \"STS-Gold Sentiment Corpus\".  [CSV is here.](https://gist.githubusercontent.com/datadave/47ed59dd8733b2063dc6/raw/583615c70a1167fcd72899b2d2830493f1c616e6/sts_gold_tweet.csv)\n",
      "\n",
      "\n",
      "Turn in a (link to your) nicely formatted notebook with your well-commented code.  You can use this and the previous assignment's code as guidelines.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}